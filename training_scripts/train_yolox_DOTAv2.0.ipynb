{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14823d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())      \n",
    "print(torch.cuda.device_count())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4c7ba",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Dataset from: https://captain-whu.github.io/DOTA/dataset.html\n",
    "Full list of albumations functions in ultralytics: https://docs.ultralytics.com/reference/data/augment/h=albumentation#ultralyticsdataaugmentBaseTransform\n",
    "\n",
    "`convert_dota_to_yolo_obb` <br>\n",
    "Converts DOTA dataset annotations to YOLO OBB (Oriented Bounding Box) format. The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory.\n",
    "\n",
    "`split_trainval` <br>\n",
    "Restructures dataset directory and applies image augmentation to enrich dataset. \n",
    "```\n",
    "data_root/\n",
    "â”œâ”€â”€ images/\n",
    "â”‚   â”œâ”€â”€ train/\n",
    "â”‚   â””â”€â”€ val/\n",
    "â””â”€â”€ labels/\n",
    "    â”œâ”€â”€ train_original/\n",
    "    â””â”€â”€ val_original/\n",
    "```\n",
    "\n",
    "And the output directory structure is:\n",
    "\n",
    "```\n",
    "save_dir/\n",
    "â”œâ”€â”€ images/\n",
    "â”‚   â”œâ”€â”€ train/\n",
    "â”‚   â””â”€â”€ val/\n",
    "â””â”€â”€ labels/\n",
    "    â”œâ”€â”€ train/\n",
    "    â””â”€â”€ val/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.converter import convert_dota_to_yolo_obb\n",
    "from ultralytics.data.split_dota import split_test, split_trainval\n",
    "\n",
    "#Run this once to convert labels and save them with images in a new directory\n",
    "#convert_dota_to_yolo_obb(\"datasets/DOTAv2.0\")\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# split train and val set, with labels.\n",
    "split_trainval(\n",
    "    data_root=\"datasets/DOTAv2.0/\",\n",
    "    save_dir=\"datasets/DOTAv2.0/processed/\",\n",
    "    rates=[0.5, 1.0, 1.5],  # Generate different versions or crops of the images\n",
    "    gap=500, # Split into patches with a specific overlap or gap\n",
    ")\n",
    "# # split test set, without labels.\n",
    "# split_test(\n",
    "#     data_root=\"DOTAv1.0/\",\n",
    "#     save_dir=\"DOTAv1.0-split/\",\n",
    "#     rates=[0.5, 1.0, 1.5],  # multiscale\n",
    "#     gap=500,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473eeef5",
   "metadata": {},
   "source": [
    "### Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d261a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"datasets/DOTAv2.0\")\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\"}\n",
    "label_exts = {\".txt\"}\n",
    "\n",
    "# Count images in each subdirectory under DOTAv2/images\n",
    "print(\"DOTAv2.0\")\n",
    "print(\"Image counts:\")\n",
    "for subdir in (root / \"images\").iterdir():\n",
    "    if subdir.is_dir():\n",
    "        count = sum(1 for file in subdir.rglob(\"*\") if file.suffix.lower() in image_exts)\n",
    "        print(f\"  {subdir.name}: {count}\")\n",
    "\n",
    "# Count labels in each subdirectory under DOTAv2/labels\n",
    "print(\"\\nLabel counts:\")\n",
    "for subdir in (root / \"labels\").iterdir():\n",
    "    if subdir.is_dir():\n",
    "        count = sum(1 for file in subdir.rglob(\"*\") if file.suffix.lower() in label_exts)\n",
    "        print(f\"  {subdir.name}: {count}\")\n",
    "        if subdir.name == \"val_original\":\n",
    "            print()  # Extra line break after 'val_original'\n",
    "\n",
    "# Repeat for DOTAv2_processed\n",
    "root2 = Path(\"datasets/DOTAv2.0/processed\")\n",
    "print(\"\\nDOTAv2_processed\")\n",
    "print(\"Image counts:\")\n",
    "for subdir in (root2 / \"images\").iterdir():\n",
    "    if subdir.is_dir():\n",
    "        count = sum(1 for file in subdir.rglob(\"*\") if file.suffix.lower() in image_exts)\n",
    "        print(f\"  {subdir.name}: {count}\")\n",
    "\n",
    "print(\"\\nLabel counts:\")\n",
    "for subdir in (root2 / \"labels\").iterdir():\n",
    "    if subdir.is_dir():\n",
    "        count = sum(1 for file in subdir.rglob(\"*\") if file.suffix.lower() in label_exts)\n",
    "        print(f\"  {subdir.name}: {count}\")\n",
    "        if subdir.name == \"val_original\":\n",
    "            print()  # Extra line break after 'val_original'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104b903",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ee99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained model\n",
    "model = YOLO(\"models/yolo11x-obb.pt\") # n for nano, x for extra-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80f71f",
   "metadata": {},
   "source": [
    "### Train model\n",
    "Includes list of augmentations to apply to each batch before training loop. Doesn't create more images, just augments the existing ones (and labels accordingly) to enrich dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52761c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data=\"datasets/DOTAv2.0/dotav2.yml\", # Path to custom dataset YAML\n",
    "    epochs=100,                          # Total number of training epochs\n",
    "    imgsz=640,                           # Input image size for training\n",
    "    patience=15,                         # patience: Number of epochs with no improvement before early stopping\n",
    "    batch=12,                            # batch: Batch size (use -1 or a fraction for auto GPU memory utilization)\n",
    "    save=True,                           # save: Save training checkpoints and final model weights\n",
    "    save_period=-1,                      # save_period: Frequency (in epochs) for saving checkpoints; -1 disables\n",
    "    cache=False,                         # cache: Enable caching of dataset images (True for RAM, 'disk' for disk caching)\n",
    "    device='0, 1',                       # device: Specify the computational device (e.g., 0, \"cpu\", or list of GPUs)\n",
    "    workers=8,                           # workers: Number of worker threads for data loading\n",
    "    project=\"test_yolo\",                 # project: Name of the project directory for saving outputs\n",
    "    name=\"experiment_1\",                 # name: Name of the training run\n",
    "    exist_ok=False,                      # exist_ok: Allow overwriting an existing project/name directory if True\n",
    "    pretrained=False,                    # pretrained: Start from a pretrained model or provide a path to one\n",
    "    optimizer=\"auto\",                    # optimizer: Choice of optimizer (e.g., 'SGD', 'Adam', or 'auto')\n",
    "    seed=18,                             # seed: Random seed for reproducibility\n",
    "    single_cls=False,                    # single_cls: Treat all classes as a single class (useful for binary tasks)\n",
    "    classes=None,                        # classes: List of class IDs to train on; None uses all classes\n",
    "    rect=False,                          # rect: Enable rectangular training to minimize padding in batches\n",
    "    multi_scale=True,                    # multi_scale: Enable multi-scale training by varying the imgsz during training\n",
    "    cos_lr=False,                        # cos_lr: Use a cosine learning rate scheduler\n",
    "    close_mosaic=10,                     # close_mosaic: Disable mosaic augmentation in the last N epochs\n",
    "    resume=False,                        # resume: Resume training from the last checkpoint if available\n",
    "    amp=True,                            # amp: Enable Automatic Mixed Precision training\n",
    "    #fraction=1.0,                       # fraction: Fraction of the dataset to use for training\n",
    "    freeze=None,                         # freeze: Freeze the first N layers or specify list of layers to freeze\n",
    "    lr0=0.01,                            # lr0: Initial learning rate\n",
    "    lrf=0.001,                           # lrf: Final learning rate as a fraction of lr0 (lr0 * lrf)\n",
    "    momentum=0.937,                      # momentum: Momentum factor for the optimizer\n",
    "    weight_decay=0.0005,                 # weight_decay: L2 regularization coefficient to prevent overfitting\n",
    "    warmup_epochs=3.0,                   # warmup_epochs: Number of epochs for learning rate warmup\n",
    "    warmup_momentum=0.8,                 # warmup_momentum: Initial momentum value during the warmup phase\n",
    "    warmup_bias_lr=0.1,                  # warmup_bias_lr: Learning rate for bias parameters during warmup\n",
    "    box=7.5,                             # box: Weight of the box loss component (bounding box regression)\n",
    "    cls=0.5,                             # cls: Weight of the classification loss component\n",
    "    dfl=1.5,                             # dfl: Weight of the distribution focal loss for fine-grained classification\n",
    "    pose=12.0,                           # pose: Weight of the pose loss for keypoint or pose estimation tasks\n",
    "    kobj=2.0,                            # kobj: Weight of the keypoint objectness loss in pose estimation models\n",
    "    nbs=64,                              # nbs: Nominal batch size for loss normalization\n",
    "    overlap_mask=True,                   # overlap_mask: Merge overlapping masks into a single mask if True\n",
    "    mask_ratio=4,                        # mask_ratio: Downsample ratio for segmentation masks\n",
    "    dropout=0.0,                         # dropout: Dropout rate for regularization\n",
    "    val=True,                            # val: Enable validation during training\n",
    "    plots=True,                          # plots: Generate and save plots of training and validation metrics\n",
    "    hsv_h=0.015,  # Adjusts the hue of the image by a fraction of the color wheel\n",
    "    hsv_s=0.7,    # Alters the saturation for varied color intensity\n",
    "    hsv_v=0.4,    # Modifies brightness to simulate different lighting conditions\n",
    "    degrees=0.0,  # Rotates the image 0Â° (no rotation); adjust between 0 and 180 for random rotation\n",
    "    translate=0.1,  # Translates the image by 10% of its size\n",
    "    scale=0.5,    # Scales the image, simulating objects at different distances\n",
    "    shear=0.0,    # Shears the image 0Â° (no shearing); modify between -180 and 180 for effect\n",
    "    perspective=0.0,  # Applies no perspective transformation; use values up to 0.001 if needed\n",
    "    flipud=0.0,   # Probability (0%) of flipping the image upside down\n",
    "    fliplr=0.5,   # 50% chance to horizontally flip the image\n",
    "    bgr=0.0,      # No conversion from RGB to BGR (0% chance)\n",
    "    mosaic=1.0,   # Enables mosaic augmentation by merging four images into one\n",
    "    mixup=0.0,    # Disables mixup augmentation (blends two images)\n",
    "    cutmix=0.0,   # Disables cutmix augmentation (combines portions of two images)\n",
    "    copy_paste=0.0,  # Disables copy-paste augmentation for segmentation\n",
    "    copy_paste_mode=\"flip\",  # Use 'flip' strategy if copy-paste is enabled\n",
    "    auto_augment=\"randaugment\",  # Applies RandAugment, a random augmentation policy for classification\n",
    "    erasing=0.4    # Randomly erases regions (40% probability) to encourage robust feature extraction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce90fdb",
   "metadata": {},
   "source": [
    "### Validated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ba0924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.127 ðŸš€ Python-3.10.17 torch-2.1.2+cu118 CUDA:0 (NVIDIA RTX 6000 Ada Generation, 48639MiB)\n",
      "YOLO11x-obb summary (fused): 199 layers, 58,756,393 parameters, 0 gradients, 202.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 532.2Â±115.9 MB/s, size: 351.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter-dai7591/yolo/datasets/DOTAv2/processed/labels/val.cache... 7122 images, 2795 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7122/7122 [00:00<?, ?it/s]\n",
      "/home/jupyter-dai7591/.conda/envs/yolo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446/446 [01:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       7122     148987      0.747      0.634      0.671      0.509\n",
      "                 plane        758       6063      0.954      0.862      0.912      0.777\n",
      "                  ship       1073      28315       0.94      0.781      0.859      0.669\n",
      "          storage tank        345       5426      0.918      0.658      0.811      0.646\n",
      "      baseball diamond        265        516      0.739      0.754      0.774      0.578\n",
      "          tennis court        261       1662      0.943       0.92      0.951       0.91\n",
      "      basketball court        138        358      0.806      0.648      0.718      0.636\n",
      "    ground track field        353        417      0.671        0.7      0.678       0.53\n",
      "                harbor        808       5719      0.888      0.699      0.794      0.521\n",
      "                bridge        529       1045      0.705      0.481      0.553      0.335\n",
      "         large vehicle        836      11317      0.837      0.739      0.802      0.629\n",
      "         small vehicle       1864      85609      0.796      0.535      0.637       0.43\n",
      "            helicopter         50        146      0.445      0.705      0.627      0.445\n",
      "            roundabout        268        370      0.819      0.586      0.659      0.523\n",
      "     soccer ball field        255        393      0.654      0.545       0.55      0.435\n",
      "         swimming pool        254       1443      0.756      0.736      0.779      0.484\n",
      "       container crane          7         30          0          0    0.00231    0.00122\n",
      "               airport        154        154      0.715      0.554      0.483      0.257\n",
      "               helipad          4          4      0.861        0.5      0.497       0.36\n",
      "Speed: 0.1ms preprocess, 6.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/obb/val3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0.77656,     0.66861,     0.64594,     0.57847,     0.90987,     0.63585,     0.52961,     0.52069,     0.33538,     0.62893,     0.43006,     0.44453,     0.52263,     0.43511,     0.48449,   0.0012233,     0.25676,     0.36031])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"../models/DOTA.pt\") # n for nano, x for huge\n",
    "metrics = model.val(data=\"../datasets/DOTAv2/DOTAv2.yml\")  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # a list contains map50-95 of each category\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
